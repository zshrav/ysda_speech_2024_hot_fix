{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbb917e",
   "metadata": {},
   "source": [
    "# Homework description\n",
    "\n",
    "In this assignment we are going to train a neural network noise reduction.\n",
    "\n",
    "The total cost of this assignment is 15 pts.\n",
    "\n",
    "## Plan:\n",
    "0. Datasets\n",
    "1. Volume normalization, gain, RMS: everything we need to mix signal with noise [2 points]\n",
    "2. Room impulse response (RIR): what we need to simulated acoustics and perform partial dereverberation [1 point]\n",
    "3. On-the-fly data generation [5 points]\n",
    "4. Neural network architecture [3 points]\n",
    "5. Loss function [1 point]\n",
    "6. Train Loop [3 points]\n",
    "7. Streaming implementation for the neural network [bonus, 3 points]\n",
    "\n",
    "## A homework submission should include:\n",
    "1. filled notebook\n",
    "2. tensorboard logs\n",
    "3. 5 examples of input-output files from the trained model in .wav format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cf343",
   "metadata": {},
   "source": [
    "# 0. Datasets\n",
    "\n",
    "We are going to use clean speech and room impluse responses from DNS Challenge dataset.\n",
    "For speech it is random subsample, for RIR we shall use the full smallroom partition to avoid extreme reverberation levels.\n",
    "\n",
    "Originally DNS Challenge data comes in 48 kHz sample rate. We [down-sampled](https://librosa.org/doc/0.10.1/generated/librosa.resample.html#librosa-resample) it to 16 kHz in advance.\n",
    "\n",
    "For noise we are going to use Musan dataset. Why not DNS Challenge? Training progress will be seen faster with Musan.\n",
    "\n",
    "Let's download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3530fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "from zipfile import ZipFile\n",
    "\n",
    "base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "public_key = 'https://disk.yandex.ru/d/ECHrgBGJrrGQqw'\n",
    "\n",
    "final_url = base_url + urlencode(dict(public_key=public_key))\n",
    "response = requests.get(final_url)\n",
    "download_url = response.json()['href']\n",
    "response = requests.get(download_url)\n",
    "\n",
    "path_to_dataset = 'data/homework_16_kHz'    # Choose any appropriate local path\n",
    "\n",
    "zipfile = ZipFile(BytesIO(response.content))\n",
    "zipfile.extractall(path=path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3550a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load_data\n",
    "\n",
    "ROOT_DATA = os.path.join(path_to_dataset, \"homework_1_16kHz\")\n",
    "\n",
    "DATA_PATHS = {\n",
    "    \"speech\": os.path.join(ROOT_DATA, \"clean_train\"),\n",
    "    \"noise\": os.path.join(ROOT_DATA, \"musan/noise\"),\n",
    "    \"rir\": os.path.join(ROOT_DATA, \"impulse_responses_all/SLR26/simulated_rirs_48k/smallroom\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772796e5",
   "metadata": {},
   "source": [
    "How many audio files do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda20ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "def list_wavs_in_folder_recursively(path: str) -> list[str]:\n",
    "    return sorted(glob(os.path.join(path, \"**\", \"*.wav\"), recursive=True))\n",
    "\n",
    "for key, folder in DATA_PATHS.items():\n",
    "    paths = list_wavs_in_folder_recursively(folder)\n",
    "    print(f\"{key}: {len(paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f853c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import librosa  # to plot mel-spectrograms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import scipy.signal as sig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "SR = 16_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spec_for_plot(waveform):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=waveform, sr=SR)\n",
    "    min_val = 1e-10\n",
    "    mel_spec = np.clip(mel_spec, min_val, None)\n",
    "    mel_spec[-1, -1] = min_val\n",
    "    mel_spec_db = 10 * np.log10(mel_spec)\n",
    "#     return mel_spec_db\n",
    "    return np.flip(mel_spec_db, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83186739",
   "metadata": {},
   "source": [
    "**Let's select sample files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"read_speech/book_00007_chp_0008_reader_01326_64_seg_2.wav\"\n",
    "SAMPLE_SIGNAL, _sr = sf.read(os.path.join(DATA_PATHS[\"speech\"], rel_path))\n",
    "\n",
    "rel_path = \"free-sound/noise-free-sound-0001.wav\"\n",
    "SAMPLE_NOISE, _sr = sf.read(os.path.join(DATA_PATHS[\"noise\"], rel_path))\n",
    "\n",
    "SAMPLE_RIR, _sr = sf.read(os.path.join(\n",
    "    ROOT_DATA,\n",
    "    \"impulse_responses_all/SLR28/RIRS_NOISES/real_rirs_isotropic_noises/air_type1_air_binaural_office_0_1_1ch.wav\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce9093",
   "metadata": {},
   "source": [
    "# 1. Volume normalization, gain, RMS: everything we need to mix signal with noise [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd65e0",
   "metadata": {},
   "source": [
    "## RMS-full-scale\n",
    "\n",
    "Let's suppose we have an audio signal $x=(x[0], x[1], ..., x[T-1])$.\n",
    "\n",
    "A basic measure of it's loundness would be its L2-norm. It is also referred to as RMS (root-mean-square):\n",
    "$$\\text{rms}_{\\text{raw}}(x) = ||x|| = ||x||_2 = \\sqrt{\\frac{1}{T}\\sum_{t=0}^{T-1} x[t]^2}$$\n",
    "\n",
    "It is convenient to express RMS in decibels. Decibels involve logarithm computation and are only applicable to dimensionless physical quantities, typically to ratios.\n",
    "\n",
    "So we need to define a reference value to normalize by. As we live in world where signals are represented with floating-point values ranged between -1 and 1, 2 options are typically adopted:\n",
    "\n",
    "$$\\text{rms}_{\\text{ref}, \\sin} = \\int_{0}^{2\\pi} sin^2(t)dt = 0.5$$\n",
    "which corresponds to rms of a sine wave , or\n",
    "$$\\text{rms}_{\\text{ref, square}} = 1$$\n",
    "the latter corresponds to rms of a [square wave](https://en.wikipedia.org/wiki/Square_wave).\n",
    "\n",
    "Both options are used, leading to confusion in the industry.\n",
    "\n",
    "In this assignment **let's use:**\n",
    "$$\\text{rms}_{\\text{ref}} = \\text{rms}_{\\text{ref, square}} = 1$$\n",
    "\n",
    "Thus we get:\n",
    "\n",
    "$$\\text{rms}_\\text{dB}(x) = 20\\log_{10}\\frac{||x||}{\\text{rms}_{\\text{ref, square}}} = 20\\log_{10}||x|| = 10\\log_{10}\\frac{1}{T}\\sum_{t=0}^{T-1}x[t]^2$$\n",
    "\n",
    "**The latter is called RMS-full-scale (or RMS-fs)**, i.e. RMS relative to full scale of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e752657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mean_square(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes mean-square of x\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "\n",
    "\n",
    "def power_to_db(x: float) -> float:\n",
    "    \"\"\"\n",
    "    Computes 10log10(x)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "\n",
    "\n",
    "def eval_rms_db(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes rms-fs of x\n",
    "    \"\"\"\n",
    "    rms_square_raw = # your code\n",
    "    rms_db = # your code\n",
    "    return rms_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5375400",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "\n",
    "assert abs(power_to_db(0.01) + 20) < eps\n",
    "assert abs(power_to_db(0.1) + 10) < eps\n",
    "assert abs(power_to_db(1) - 0) < eps\n",
    "assert abs(power_to_db(10) - 10) < eps\n",
    "assert abs(power_to_db(100) - 20) < eps\n",
    "\n",
    "assert abs(eval_rms_db(np.ones(999) * 0.1) + 20) < eps\n",
    "assert abs(eval_rms_db(np.ones(999) * 1) - 0) < eps\n",
    "assert abs(eval_rms_db(np.ones(531) * 10) - 20) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2916b",
   "metadata": {},
   "source": [
    "## Gain, normalization\n",
    "\n",
    "When a signal $x$ is multiplied by a scalar factor of $\\alpha \\geq 0$, it corresponds to addition in the world of decibels:\n",
    "$$\\text{rms}_\\text{dB}(\\alpha x) = 20 \\log_{10} ||\\alpha x|| = 20\\log_{10}\\alpha + 20\\log_{10} ||x|| = 20\\log_{10}\\alpha + \\text{rms}_\\text{dB}(x)$$\n",
    "\n",
    "The multiplication of $x$ by $\\alpha \\geq 0$ is often referred to as **gain by $\\boldsymbol{G}$ dB**, where $G = 20\\log_{10}\\alpha$, which can be both positive or negative (or even infinite negative if $\\alpha=0$).\n",
    "\n",
    "The inverse relationship can be inferred to express the scalar factor from the gain in decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79065005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_to_mult(gain_db: float) -> float:\n",
    "    \"\"\"\n",
    "    Finds the positive scalar factor which corresponds to given gain_db\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")\n",
    "    \n",
    "    \n",
    "def mult_to_gain(mult: float) -> float:\n",
    "    \"\"\"\n",
    "    Finds the gain in dB from positive scalar factor\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-12\n",
    "\n",
    "assert abs(gain_to_mult(-20) - 0.1) < eps\n",
    "assert abs(gain_to_mult(0) - 1) < eps\n",
    "assert abs(gain_to_mult(20) - 10) < eps\n",
    "\n",
    "for mult in [1., 0.265, 10.5]:\n",
    "    gain_db = mult_to_gain(mult)\n",
    "    mult_power = mult ** 2  # if x is multiplied by mult, x ** 2 is multiplied by mult ** 2\n",
    "    gain_db_from_power = power_to_db(mult_power)\n",
    "    assert abs(gain_db_from_power - gain_db) < eps\n",
    "    mult_restored = gain_to_mult(gain_db)\n",
    "    assert abs(mult - mult_restored) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234f5a4",
   "metadata": {},
   "source": [
    "Now we know how to apply gain in decibels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gain(x: np.ndarray, gain_db: float) -> np.ndarray:\n",
    "    mult = # your code\n",
    "    result = # your code\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0e4a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(10)\n",
    "\n",
    "assert np.allclose(apply_gain(x, -20), x / 10)\n",
    "assert np.allclose(apply_gain(x, 0), x)\n",
    "assert np.allclose(apply_gain(x, 20), x * 10)\n",
    "\n",
    "\n",
    "for _ in range(100):\n",
    "    x = np.random.uniform(-1, 1, 16_000)\n",
    "    gain = np.random.uniform(-20, 20)\n",
    "    rms_before = eval_rms_db(x)\n",
    "    rms_expected = rms_before + gain\n",
    "    \n",
    "    gained = apply_gain(x, gain)\n",
    "    rms_after = eval_rms_db(gained)\n",
    "    \n",
    "    assert abs(rms_after - rms_expected) < 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcc8dc",
   "metadata": {},
   "source": [
    "Let's implement a function which will normalize an input signal to a desired level of $\\text{rms}_\\text{dB}$.\n",
    "\n",
    "How should it work?\n",
    "\n",
    "1. Calculate $\\text{rms}_\\text{dB}(\\text{signal})$\n",
    "2. Calculate gain in decibels: $g = \\text{rms}_\\text{dB, target} - \\text{rms}_\\text{dB}(\\text{signal})$\n",
    "3. Apply the gain to the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246907b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_rms(x: np.ndarray, target_rms_db: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes signal x to target_rms_db\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    x = np.random.uniform(-1, 1, 16_000)\n",
    "    target_rms_db = np.random.uniform(-20, 20)\n",
    "    normalized = normalize_to_rms(x, target_rms_db)\n",
    "    rms_after_normalization = eval_rms_db(normalized)\n",
    "    assert abs(target_rms_db - rms_after_normalization) < 1e-12\n",
    "    \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea09b1",
   "metadata": {},
   "source": [
    "**Let's play with it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ebb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.copy(SAMPLE_SIGNAL)\n",
    "\n",
    "gain_db = -10\n",
    "x_gained = apply_gain(x, gain_db)\n",
    "x_normalized = normalize_to_rms(x, -20)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(x_normalized, label=\"rms: -20 dB\")\n",
    "ax.plot(x, label=\"raw\")\n",
    "ax.plot(x_gained, label=f\"gained by {gain_db} dB\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e5e60",
   "metadata": {},
   "source": [
    "### SNR\n",
    "\n",
    "SNR (signal-to-noise ratio) is expressed in decibels and is defined as:\n",
    "\n",
    "$$\\text{SNR} = 10\\log_{10}\\frac{||\\text{signal}||^2}{||\\text{noise}||^2} = 10\\log_{10}||\\text{signal}||^2 - 10\\log_{10}||\\text{noise}||^2 = \\text{rms}_{\\text{dB}}(\\text{signal}) - \\text{rms}_{\\text{dB}}(\\text{noise})$$\n",
    "\n",
    "Also, **SNR can be used as a quality metrics** or even a loss function for gradient descent.\n",
    "\n",
    "Given a ground truth signal $y$ and its estimate $\\hat y$, we define noise as $\\hat y - y$. Slightly abusing notation we get:\n",
    "\n",
    "$$\\text{SNR}(\\hat y, y) = 10 \\log_{10} \\frac{||\\hat y - y||^2}{||y||^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_snr(estimate: np.ndarray, signal: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    evaluates SNR as a quality metrics\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Your code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ad597",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "\n",
    "    signal = np.random.uniform(-1, 1, 16_000)\n",
    "    noise = np.random.uniform(-1, 1, 16_000) * np.random.uniform(0.3, 2)\n",
    "    mixture = signal + noise\n",
    "    snr = eval_rms_db(signal) - eval_rms_db(noise)\n",
    "    snr_est = eval_snr(mixture, signal)\n",
    "    assert abs(snr - snr_est) < 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffebcc7",
   "metadata": {},
   "source": [
    "Now we have everything to generate a mixture of speech and noise with defined signal loudness (RMS) and SNR:\n",
    "\n",
    "1. Normalize signal to $\\text{rms}_\\text{target, signal}$\n",
    "2. Noramlize noise to $\\text{rms}_\\text{target, noise} = \\text{rms}_\\text{target, signal} - \\text{SNR}$\n",
    "3. Add noise to signal. What if shapes don't match? Let's just assume they match and enforce it outside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_speech_with_noise(signal, noise, rms_signal, snr):\n",
    "    signal_normalized = # your code\n",
    "    rms_noise_target = # your code\n",
    "    noise_normalized = # your code\n",
    "    mixture = signal + noise\n",
    "    return mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    signal = np.random.uniform(-1, 1, 16_000)\n",
    "    noise = np.random.uniform(-1, 1, 16_000) * np.random.uniform(0.3, 2)\n",
    "    rms_signal = np.random.uniform(-20, 20)\n",
    "    snr = np.random.uniform(-20, 20)\n",
    "    mixture = mix_speech_with_noise(signal, noise, rms_signal, snr)\n",
    "    \n",
    "    signal_gained = normalize_to_rms(signal, rms_signal)\n",
    "    snr_est = eval_snr(mixture, signal_gained)\n",
    "\n",
    "    assert abs(snr - snr_est) < 1e-12\n",
    "    \n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb751a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr in [-10, 10]:\n",
    "    min_len = min(len(SAMPLE_SIGNAL), len(SAMPLE_NOISE))\n",
    "    signal = SAMPLE_SIGNAL[:min_len]\n",
    "    noise = SAMPLE_NOISE[:min_len]\n",
    "\n",
    "    mixture = mix_speech_with_noise(signal, noise, -20, snr)\n",
    "\n",
    "    spec = build_spec_for_plot(mixture)\n",
    "    _, ax = plt.subplots()\n",
    "    ax.set_title(f\"SNR: {snr} dB\")\n",
    "    ax.imshow(spec)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a6d8f",
   "metadata": {},
   "source": [
    "# 2. Room impulse response (RIR): what we need to simulated acoustics and perform partial dereverberation [1 point]\n",
    "\n",
    "The common approach to simulate acoustics is convolving signal with room impulse response (RIR).\n",
    "\n",
    "It follows from the linear acoustic model (which is accurate enough to be used in practice) and the assumption of time-invariance (i.e. that room acoustics does not change over time or it changes slowly).\n",
    "\n",
    "For input signal $x$ and RIR $r$:\n",
    "\n",
    "$$x_{\\text{reverberated}} = x * r$$\n",
    "\n",
    "A RIR is defined as the reverberated version of the unit impulse, i.e. the (1, 0, 0, 0, ...) signal.\n",
    "\n",
    "An RIR can be listened to and it sounds like a click."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390a90c",
   "metadata": {},
   "source": [
    "**Let's take a look at an impulse response.**\n",
    "\n",
    "This will be a real impulse response from a relatively highly reverberant environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d4d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rir = np.copy(SAMPLE_RIR)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(rir)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37716d76",
   "metadata": {},
   "source": [
    "**This is how RIR is convolved with a signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved = sig.convolve(SAMPLE_SIGNAL, SAMPLE_RIR, mode=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(nrows=2, figsize=(16, 7), sharex=True, sharey=True)\n",
    "for ax, (name, data) in zip(axes, [\n",
    "    (\"raw\", SAMPLE_SIGNAL),\n",
    "    (\"convolved\", convolved),\n",
    "]):\n",
    "    ax.set_title(name)\n",
    "    ax.plot(data)\n",
    "    ax.grid()\n",
    "plt.show()\n",
    "\n",
    "_, axes = plt.subplots(nrows=2, figsize=(16, 7), sharex=True, sharey=True)\n",
    "for ax, (name, data) in zip(axes, [\n",
    "    (\"raw\", SAMPLE_SIGNAL),\n",
    "    (\"convolved\", convolved),\n",
    "]):\n",
    "    spec = build_spec_for_plot(data)\n",
    "    ax.set_title(name)\n",
    "    ax.imshow(spec)\n",
    "    ax.set_aspect(\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cd8d0",
   "metadata": {},
   "source": [
    "**Note the shapes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(convolved) == len(SAMPLE_SIGNAL) + len(SAMPLE_RIR) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293346d0",
   "metadata": {},
   "source": [
    "The output file is longer, and as it can be seen from the spectrum, the appended length maingly consists of reverberation tail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb22c8",
   "metadata": {},
   "source": [
    "**Let's take a closer look what a RIR looks like**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a6ecf",
   "metadata": {},
   "source": [
    "**These are helper functinons** which evaluate windowed power of a signal (RIR) and plot it in the dB scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1002791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_power(rir, win_size=160):\n",
    "    win = np.hanning(win_size)\n",
    "    win /= win.sum()\n",
    "    rir_sq = np.square(rir)\n",
    "    win_power = sig.convolve(rir_sq, win, mode=\"valid\")\n",
    "    return win_power\n",
    "\n",
    "\n",
    "def plot_win_power_db(win_power_db, ax):\n",
    "    lines = ax.plot(win_power_db)\n",
    "    return lines[0]\n",
    "\n",
    "\n",
    "def plot_rir_for_rt60(rir, ax):\n",
    "    win_power = get_win_power(rir)\n",
    "    win_power_db = 10 * np.log10(win_power)\n",
    "    line = plot_win_power_db(win_power_db, ax)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "win_power = get_win_power(rir)\n",
    "win_power_db = 10 * np.log10(win_power)\n",
    "plot_win_power_db(win_power_db, ax)\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2e880",
   "metadata": {},
   "source": [
    "**We can oberve the following pattern:**\n",
    "\n",
    "First the power drops abruptly and then it decays by a linear pattern in the log scale.\n",
    "\n",
    "The slope of the linear fit defines the $\\boldsymbol{rt_{60}}$ property of a RIR (and even a room).\n",
    "\n",
    "$\\boldsymbol{rt_{60}}$ (reverb time 60) is the time in which the linear fit decays by 60 dB. Measured in seconds.\n",
    "\n",
    "Why 60 dB? It is the difference between the loudest and the quietest volumes in a symphonic orchestra.\n",
    "\n",
    "**For the curious:** [More about rt60](https://svantek.com/academy/rt60-reverberation-time/), they measure it directly, not from an RIR.\n",
    "\n",
    "**Let's fit a linear regression estimator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1824302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "linear_trend_start = 500\n",
    "linear_trend_end = 8000\n",
    "\n",
    "x = np.arange(linear_trend_end)[linear_trend_start: linear_trend_end]\n",
    "y = win_power_db[linear_trend_start: linear_trend_end]\n",
    "\n",
    "linear_regression.fit(x[:, None], y);\n",
    "linear_fit = linear_regression.predict(x[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39dd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "plot_rir_for_rt60(rir, ax)\n",
    "ax.plot(x, linear_fit)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e517be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = linear_regression.coef_.item()\n",
    "# intercept = linear_regression.intercept_\n",
    "rt_60_sec = # your code: use coef to evaluate rt60\n",
    "\n",
    "assert abs(rt_60_sec - 0.7) < 0.02, rt_60_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3798dfe",
   "metadata": {},
   "source": [
    "**RIR decay**\n",
    "\n",
    "To prepare targets for partial dereverberation RIR is decayed.\n",
    "\n",
    "How we will do it:\n",
    "\n",
    "1. Find the argmax of a RIR and keep the next 20 ms as well as the part before argmax unchanged. This part of RIR corresponds to direct sound and early reverberation.\n",
    "\n",
    "2. The rest part should be decayed exponentially, -60 dB per 0.3 sec.\n",
    "\n",
    "This is something between the way it is done in [PoCoNet](https://arxiv.org/pdf/2008.04470.pdf) and [Cruse](https://arxiv.org/pdf/2101.09249.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb79b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_rir(rir: np.ndarray, decay_rt_60_sec = 0.3, sr=SR) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decays a RIR as described above\n",
    "    \"\"\"\n",
    "    main_tap = np.argmax(rir).item()\n",
    "    early_reverb_duration_sec = 0.020\n",
    "    early_reverb_duration_frames = int(early_reverb_duration_sec * sr)\n",
    "\n",
    "    # your code\n",
    "    return rir_decayed\n",
    "\n",
    "\n",
    "rir_decayed = decay_rir(rir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df91be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(\"RIR power decay\")\n",
    "ax.set_ylabel(\"Power (db-fs)\")\n",
    "plot_rir_for_rt60(rir, ax)\n",
    "plot_rir_for_rt60(rir_decayed, ax)\n",
    "\n",
    "ax.set_ylim(-120, -23)\n",
    "x_ticks = ax.get_xticks()\n",
    "x_tick_labels = x_ticks / SR\n",
    "ax.set_xticks(x_ticks, x_tick_labels)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0fa52",
   "metadata": {},
   "source": [
    "We don't provide and assertion test here, but this is what it should look like.\n",
    "\n",
    "Pay attention to your decay rate, it should not deviate too much.\n",
    "\n",
    "![title](assets/pictures/rir_decayed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ddce28",
   "metadata": {},
   "source": [
    "# 3. On-the-fly data generation [5 points]\n",
    "\n",
    "**Why?**\n",
    "\n",
    "We are going to train a model on synthetic mixtures of signals, noises with acoustics simulation via RIR convolution.\n",
    "\n",
    "Why do we train a model on synthetic data? It is the most straight-forward way to obtain corresponding (mixture, signal) pairs.\n",
    "\n",
    "It can seem natural to simulate all the data in advance and train on it.\n",
    "\n",
    "But we shall take another approach: we will generate training mixtures on-the-fly. Data will be generated in parallel with forward-backward passes on GPU -- thanks to PyTorch's DataLoader class.\n",
    "\n",
    "Generating data on the fly we can both increate training data diversity and save disk storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd534876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqe.data.sampling import list_wavs_in_folder, SignalSampler, RirSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041490c",
   "metadata": {},
   "source": [
    "**Efficent audio chunk reading:**\n",
    "\n",
    "We are going to train on fixed-length chunks of audio.\n",
    "\n",
    "A naive approach to read a chunk of audio file would be to read the full file and then crop it.\n",
    "\n",
    "But we can do it better.\n",
    "\n",
    "`sf.read` function provides `start` and `stop` arguments. When provided, `audio[start: stop]` is read directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"read_speech/book_00002_chp_0005_reader_11980_15_seg_1.wav\"\n",
    "path = os.path.join(DATA_PATHS[\"speech\"], rel_path)\n",
    "\n",
    "x, _sr = sf.read(path)\n",
    "print(f\"total duration: {len(x)} frames or {len(x) / SR} seconds\")\n",
    "\n",
    "crop_size_sec = 1\n",
    "crop_size_frames = int(crop_size_sec * SR)\n",
    "\n",
    "start = 16_000\n",
    "stop = start + crop_size_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "x = sf.read(path)[0][start: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "x, _sr = sf.read(path, start=start, stop=stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24013e",
   "metadata": {},
   "source": [
    "**Let's implement a class that will read raw signal and noise data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19887d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalSampler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        paths: list[str],\n",
    "        crop_size_sec: float = 5.0,\n",
    "        min_rms_db: float | None = -38,\n",
    "        sr: int = SR,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        paths: list of absolute paths to the files we are going to sample from\n",
    "        crop_size_sec: the size of generated chunks, in seconds\n",
    "        min_rms_db: chunks with RMS lower than this should be discarded\n",
    "        sr: samplerate\n",
    "        \"\"\"\n",
    "        self.paths = paths\n",
    "        self.crop_size_frames = int(crop_size_sec * sr)\n",
    "        self.min_rms_db = min_rms_db\n",
    "        self.sr = sr\n",
    "\n",
    "    def _sample_from_single_file(\n",
    "        self, path: str, crop_size_frames: int | None = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reads a random crop of size crop_size_frames from path.\n",
    "        If the file is shorter, reads the full file.\n",
    "           \n",
    "        Use sf.read(..., start=start_index, stop=end_index) to read\n",
    "        chunks efficiently.\n",
    "        \"\"\"\n",
    "        if crop_size_frames is None:\n",
    "            crop_size_frames = self.crop_size_frames\n",
    "        with sf.SoundFile(path) as f:\n",
    "            if f.samplerate != self.sr:\n",
    "                assert False, (path, f.samplerate, self.sr)\n",
    "            file_duration_frames: int = f.frames\n",
    "        if file_duration_frames < crop_size_frames:\n",
    "            # your code\n",
    "            return # your code\n",
    "        # your code; keep in mind the corner case when file_duration_frames == crop_size_frames\n",
    "        x, _sr = sf.read(path, start=..., stop=...)\n",
    "        return x\n",
    "\n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generates a chunk of audio data of length self.crop_size_frames.\n",
    "        \n",
    "        1. Samples a random file from self.paths\n",
    "        \n",
    "        2. Reads its random crop of the target size (initialized as self.crop_size_frames).\n",
    "           If the file is shorter, reads the full file.\n",
    "           <this should be done in self._sample_from_single_file>\n",
    "           \n",
    "        3. Checks RMS of the crop.\n",
    "           The crop is discarded if its rms is lower than self.min_rms_db.\n",
    "           Otherwise it is accumulated\n",
    "           \n",
    "        4. Returns the concatenation of accumulated crops\n",
    "           if their total length reaches self.crop_size_frames.\n",
    "           Otherwise sets target size (for 2) to n_frames_ramaining and repeats 1-4\n",
    "        \"\"\"\n",
    "        chunks: list[np.ndarray] = []\n",
    "        duration_frames_remaining = self.crop_size_frames\n",
    "        while duration_frames_remaining > 0:\n",
    "            path = # your code: sample a random path\n",
    "            chunk = self._sample_from_single_file(path, duration_frames_remaining)\n",
    "            if self.min_rms_db is not None:\n",
    "                # your code\n",
    "                \n",
    "                if chunk_rms_db < self.min_rms_db:\n",
    "                    continue\n",
    "            # your code\n",
    "        result = np.concatenate(chunks)\n",
    "\n",
    "        assert result.ndim == 1, result.shape\n",
    "        assert len(result) == self.crop_size_frames\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop_size_sec in [1, 3]:\n",
    "    for key in [\"speech\", \"noise\"]:\n",
    "        print(f\"crop_size_sec: {crop_size_sec}, data: {key}\")\n",
    "        sampler = SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[key]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        )\n",
    "        n_samples = 1000\n",
    "        for idx in enumerate(tqdm(range(n_samples))):\n",
    "            chunk = sampler()\n",
    "            assert chunk.ndim == 1, chunk.shape\n",
    "            assert len(chunk) == crop_size_sec * SR, chunk.shape\n",
    "        print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfea266",
   "metadata": {},
   "source": [
    "**Now let's define a similar sampler for RIRs.**\n",
    "\n",
    "This guy is simpler, because it does not read chunks: it should read full RIRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RirSampler:\n",
    "    def __init__(self, paths: list[str], sr: int = SR) -> None:\n",
    "        \"\"\"\n",
    "        paths: list of absolute paths to the files we are going to sample from\n",
    "        sr: samplerate\n",
    "        \"\"\"\n",
    "        self.paths = paths\n",
    "        self.sr = sr\n",
    "\n",
    "    def __call__(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Samples a random path and reads the full audio file from it\n",
    "        \"\"\"\n",
    "        # your code\n",
    "        assert sr == self.sr, (path, sr, self.sr)\n",
    "        return rir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011353ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RirSampler(list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]))\n",
    "n_samples = 1000\n",
    "for idx in enumerate(tqdm(range(n_samples))):\n",
    "    rir = sampler()\n",
    "    assert rir.ndim == 1, rir.shape\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2635a2",
   "metadata": {},
   "source": [
    "**We have learnt to sample RIRs and chunks of signal/noise. Now's the time to learn how to mix them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_same_length(x: np.ndarray, rir: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convoves signal with rir and crops the result to have the original shape\n",
    "    \"\"\"\n",
    "    convolved = sig.convolve(x, rir, mode=\"full\")\n",
    "    result = convolved[: len(x)]  # we crop out the reverb-only semgent\n",
    "    return result\n",
    "\n",
    "\n",
    "class RandomMixtureSampler:\n",
    "    \"\"\"\n",
    "    Inspired by PoCoNet: https://arxiv.org/pdf/2008.04470.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sig_sampler: tp.Callable[[], np.ndarray],  # SignalSampler\n",
    "        noise_sampler: tp.Callable[[], np.ndarray],  # SignalSampler\n",
    "        rir_sampler: tp.Callable[[], np.ndarray],  # RirSampler\n",
    "        prob_rir_sig: float = 0.5,  # prob to convolve signal with a RIR\n",
    "        prob_rir_noise: float = 0.5,  # prob to convolve noise with a RIR\n",
    "        normalization_rms_db: float = -20,  # normalization level used for signal and noise before gain\n",
    "        noise_gain_range_db: tuple[float, float] = (-5, 5),  # gain applied to noise\n",
    "        mixture_gain_range_db: tuple[float, float] = (-25, 5),  # gain applied to final mixture\n",
    "        partial_dereverb: bool = True,  # whether to do partial dereverberation\n",
    "        *,\n",
    "        sr: int = 16_000,  # samplerate\n",
    "    ) -> None:\n",
    "        self.sig_sampler = sig_sampler\n",
    "        self.noise_sampler = noise_sampler\n",
    "        self.rir_sampler = rir_sampler\n",
    "        self.prob_rir_sig = prob_rir_sig\n",
    "        self.prob_rir_noise = prob_rir_noise\n",
    "        self.sr = sr\n",
    "\n",
    "        self.normalization_rms_db = normalization_rms_db\n",
    "        self.noise_gain_range_db = noise_gain_range_db\n",
    "        self.mixture_gain_range_db = mixture_gain_range_db\n",
    "\n",
    "        self.partial_dereverb = partial_dereverb\n",
    "\n",
    "    def sample_noise_rms_db(self) -> float:\n",
    "        \"\"\"\n",
    "        Samples the rms_db for noise which is relevant before mixing with signal.\n",
    "\n",
    "        Noise is first normlized to normalization_rms_db\n",
    "        and the gain by Uniform(self.noise_gain_range_db).\n",
    "\n",
    "        These 2 operations can be implemented as a single normalize_to_rms operation\n",
    "        with the final rms.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"your code\")\n",
    "\n",
    "    def sample_mixture_gain(self) -> float:\n",
    "        \"\"\"\n",
    "        Uniform(*self.mixture_gain_range_db)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"your code\")\n",
    "\n",
    "    def __call__(self) -> tuple[np.ndarray, np.ndarray]:\n",
    "        signal = self.sig_sampler()\n",
    "        noise = self.noise_sampler()\n",
    "        if np.random.binomial(1, self.prob_rir_sig):\n",
    "            rir_signal = # your code\n",
    "            signal_input = # your code: signal which will be part of the input, not the target\n",
    "            if self.partial_dereverb:\n",
    "                rir_signal_decayed = # your code\n",
    "                signal_target = # your code: target signal\n",
    "            else:\n",
    "                signal_target = np.copy(\n",
    "                    signal_input\n",
    "                )  # np.copy is crucial to avoid double scaling\n",
    "        else:\n",
    "            signal_input = signal\n",
    "            signal_target = np.copy(\n",
    "                signal\n",
    "            )  # np.copy is crucial to avoid double scaling\n",
    "        del signal\n",
    "        if np.random.binomial(1, self.prob_rir_noise):\n",
    "            rir_noise = # your code\n",
    "            noise =  # your code\n",
    "\n",
    "        # input_signal and mic_signal should be multiplied by the same factor to match each other\n",
    "        mult_signal = normalize_to_rms(\n",
    "            signal_target, self.normalization_rms_db\n",
    "        )\n",
    "        signal_input *=  # your code\n",
    "        signal_target *=  # your code\n",
    "\n",
    "        noise_rms_db = self.sample_noise_rms_db()\n",
    "        mult_noise = normalize_to_rms(noise, noise_rms_db)\n",
    "        noise *=  # your code\n",
    "\n",
    "        mixture = signal_input + noise\n",
    "\n",
    "        mixture_gain_db = self.sample_mixture_gain()\n",
    "        mixture_mult =  # your code\n",
    "\n",
    "        mixture *=  # your code\n",
    "        signal_target *=  # your code: target should be scaled with mixture for them to match each other\n",
    "\n",
    "        mixture = mixture.astype(np.float32)\n",
    "        signal_target = signal_target.astype(np.float32)\n",
    "\n",
    "        return mixture, signal_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ef2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqe.data.mixing import RandomMixtureSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b430237",
   "metadata": {},
   "outputs": [],
   "source": [
    "for crop_size_sec in [1, 5]:\n",
    "    print(f\"crop_size_sec: {crop_size_sec}\")\n",
    "    sampler = RandomMixtureSampler(\n",
    "        sig_sampler=SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        ),\n",
    "        noise_sampler=SignalSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "            crop_size_sec=crop_size_sec\n",
    "        ),\n",
    "        rir_sampler=RirSampler(\n",
    "            list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"])\n",
    "        ),\n",
    "    )\n",
    "    n_samples = 1000\n",
    "    for idx in enumerate(tqdm(range(n_samples))):\n",
    "        mixture, signal = sampler()\n",
    "        assert len(mixture) == len(signal) == crop_size_sec * SR, chunk.shape\n",
    "    print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4171e",
   "metadata": {},
   "source": [
    "**Sanity check**\n",
    "\n",
    "It is easy to leave bugs with gains. Here is a simple way to check it.\n",
    "\n",
    "We should listen to: mixture, signal and the difference: (mixture - signal).\n",
    "\n",
    "The difference is the sum of noise and late reverberation. No distinct signal should stay there.\n",
    "\n",
    "An even simpler sanity check: turn partial dereverberation off in the sampler. Then difference should be the noise and it should not contain any trace of the speech signal.\n",
    "\n",
    "Let's generate some tracks in 2 modes:\n",
    "1. No-dereverb\n",
    "2. Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(sampler, n_samples=4):\n",
    "    _, axes = plt.subplots(ncols=3, nrows=n_samples, figsize=(16, 10))\n",
    "    for sample_idx in range(n_samples):\n",
    "        mixture, target = sampler()\n",
    "        interference = mixture - target\n",
    "\n",
    "        spec_mixture = build_spec_for_plot(mixture)\n",
    "        spec_target = build_spec_for_plot(target)\n",
    "        spec_interf = build_spec_for_plot(interference)\n",
    "\n",
    "        ax = axes[sample_idx][0]\n",
    "        ax.imshow(spec_mixture)\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "        ax = axes[sample_idx][1]\n",
    "        ax.imshow(build_spec_for_plot(target))\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "        ax = axes[sample_idx][2]\n",
    "        ax.imshow(build_spec_for_plot(interference))\n",
    "        ax.set_aspect(\"auto\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size_sec = 5\n",
    "\n",
    "sig_sampler = SignalSampler(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "    crop_size_sec=crop_size_sec\n",
    ")\n",
    "noise_sampler = SignalSampler(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "    crop_size_sec=crop_size_sec\n",
    ")\n",
    "rir_sampler = RirSampler(list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]))\n",
    "\n",
    "print(\"dereverb off:\")\n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    "    partial_dereverb=False,\n",
    ")\n",
    "show_samples(sampler)\n",
    "print(\"*\" * 50)\n",
    "\n",
    "print(\"Full\")\n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    ")\n",
    "show_samples(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c632f95",
   "metadata": {},
   "source": [
    "**Wrapping our sampler to PyTorch Dataset**\n",
    "\n",
    "On `__getitem__` it will ignore the input and return a sampler from the sampler.\n",
    "We also define `dummy_duration` variable which will simulate the size of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfacfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "\n",
    "\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, sampler: RandomMixtureSampler, dummy_duration: int):\n",
    "        self.sampler = sampler\n",
    "        self.dummy_duration = dummy_duration\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dummy_duration\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Ignores index and a sample from self.sampler, converted to float32\n",
    "        \"\"\"\n",
    "        mixture, target = self.sampler()\n",
    "        mixture = mixture.astype(np.float32)\n",
    "        target = target.astype(np.float32)\n",
    "        return mixture, target\n",
    "\n",
    "    \n",
    "sampler = RandomMixtureSampler(\n",
    "    sig_sampler=sig_sampler,\n",
    "    noise_sampler=noise_sampler,\n",
    "    rir_sampler=rir_sampler,\n",
    ")\n",
    "dataset = Dataset(sampler, 100_000)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1532314c",
   "metadata": {},
   "source": [
    "Now we can use PyTorch DataLoader with our sampler, which should be really fast. If the throughput is higher that 10 batches per second (note that it outputs batches, not single samples), it is more than enough.\n",
    "\n",
    "Pay attention to the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95733386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"setting different numpy seeds for different workers\"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "    \n",
    "    \n",
    "loader = Data.DataLoader(\n",
    "    dataset, batch_size=10, num_workers=8,\n",
    "    worker_init_fn=worker_init_fn\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader)):\n",
    "    if idx == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8629c9d",
   "metadata": {},
   "source": [
    "**Data is ready**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0a086",
   "metadata": {},
   "source": [
    "# 4. Neural Network Architecture [3 points]\n",
    "\n",
    "**Our network will be a 2D UNet operating in STFT domain.**\n",
    "\n",
    "It will implement the Complex Spectral Mapping scheme (i.e. complex spectrum input -- complex spectrum output) with 2 decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio as tha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3531079",
   "metadata": {},
   "source": [
    "Let's implement [causal convolution](https://paperswithcode.com/method/causal-convolution).\n",
    "\n",
    "For a moment we may think of it as of a 1D convolution.\n",
    "\n",
    "Imagine we have an input: [1, 2, 3, 4, 5, 6, 7]\n",
    "and our kernel size is 3, let's say the kernel is [1/3, 1/3, 1/3].\n",
    "\n",
    "How will convolution process the input without padding? The input will be chunked into frames: [[1, 2, 3], [2, 3, 4], [4, 5, 6], [5, 6, 7]] and for each frame dot-product with the kernel will be calculated, resuling into \n",
    "[(1/3 + 2/3 + 3/3), (2/3 + 3/3 + 4/3), (4/3, 5/3, 6/3), (5/3, 6/3, 7/3)] = [2, 3, 4, 5, 6].\n",
    "\n",
    "With a causal convolution we want every i-th chunk to end exactly on position i, i.e. we want the chunks to be:\n",
    "[[\\*, \\*, 1], [\\*, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]] where the stars mean it's unclear what to place on their positions.\n",
    "\n",
    "These chunks correspond the convolution without padding for the following input: [\\*, \\*, 1, 2 ,3, 4, 5, 6, 7].\n",
    "And it is exactly a convolution with padding equal to (kernel_size - 1, 0). For the stars, let's use zero padding.\n",
    "\n",
    "For a 2D convolution, the idea does not change, because it should only be causal in 1 dimention, which corresponds to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1D(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A sequential of ConstantPad1d and Conv1d.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        dilation: int,\n",
    "    ):\n",
    "        # your code\n",
    "        padding_layer = # your code\n",
    "        conv_layer = # your code\n",
    "        super().__init__(\n",
    "            padding_layer,\n",
    "            conv_layer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa01928",
   "metadata": {},
   "source": [
    "Let's implement Double Modified Gated TCM (the guy with Primal and Dual domains; note that is also employs residual connection which is not shown in the picture) as it is defined in [the paper](https://arxiv.org/pdf/2011.01561.pdf)\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"assets/pictures/tcm.png\" alt=\"MgTcm\" width=\"200\"/>\n",
    "  <img src=\"assets/pictures/tcm_double.png\" alt=\"DMgTcm\" width=\"400\"/>\n",
    "</p>\n",
    "\n",
    "<!-- <p float=\"left\">\n",
    "  <img src=\"assets/pictures/tcm.png\" alt=\"MgTcm\" width=\"200\"/>\n",
    "  <img src=\"assets/pictures/tcm_double.png\" alt=\"DMgTcm\" width=\"200\"/>\n",
    "</p> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedBranch(nn.Module):\n",
    "    def __init__(self, dilation: int, n_channels: int = 64, kernel_size: int = 5):\n",
    "        super().__init__(self):\n",
    "        # your code\n",
    "\n",
    "\n",
    "class DoubleModifiedGatedTcm(nn.Module):\n",
    "    \"\"\"\n",
    "    MgTcm here\n",
    "    https://arxiv.org/pdf/2011.01561.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dilation: int, c_in: int = 256, c_hidden: int = 64, ):\n",
    "        super().__init__()\n",
    "        # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6f5ea",
   "metadata": {},
   "source": [
    "Let's build a 2D UNet with the following parameters:\n",
    "- num encoder-decoder layer sets: 5\n",
    "- num decoders: 2: 1 for real part and 1 for imaginary part of complex spectrogram\n",
    "- for skip connection let's use addition through a point-wise conv (i.e. kernel_size=1) instead of concatenation. 1 point-wise code for each of the 2 decoders.\n",
    "- kernel_size: (1, 4): 1 for the time dimension and 4 if freq dimension. With time dimension size of 1 a convolution with allways be causal.\n",
    "- stride: (1, 2): 1 for time, 2 for frequencies. A causal model may not use stride != 1 in time dimension\n",
    "- num out channels: 64 the final output layers\n",
    "- middle: TCM (i.e. reshape 2D, compressing channels and frequencies to a single dimension, linear projection, TCM, linear projection, reshape 1D to 2D\n",
    "\n",
    "TCM parameters:\n",
    "\n",
    "- dilations: (1, 2, 4, 8, 16, 32) x 2\n",
    "- layer: Double Modified Gated TCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e24dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEngine(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: complex spectrum of shape (batch_size, 2, time, n_frequencies=161)\n",
    "    Output: Similar\n",
    "    \"\"\"\n",
    "    # your code\n",
    "    \n",
    "    def forward(self, spec):\n",
    "        assert spec.ndim == 4, spec.shape\n",
    "        assert spec.shape[1] == 2, (spec.shape, \"complex\")\n",
    "        # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c4e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetEngine()\n",
    "\n",
    "x = torch.rand(3, 2, 298, 161)\n",
    "out = model(x)\n",
    "assert x.shape == out.shape, (x.shape, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexSpectumMappingModel(nn.Module):\n",
    "    def __init__(self, engine):\n",
    "        super().__init__()\n",
    "        self.stft = tha.transforms.Spectrogram(\n",
    "            n_fft=320,\n",
    "            win_length=320,\n",
    "            hop_length=160,\n",
    "            power=None,\n",
    "            window_fn=torch.hann_window,\n",
    "        )\n",
    "\n",
    "        self.istft = tha.transforms.InverseSpectrogram(\n",
    "            n_fft=320,\n",
    "            win_length=320,\n",
    "            hop_length=160,\n",
    "            window_fn=torch.hann_window,\n",
    "        )\n",
    "        self.engine = engine\n",
    "        \n",
    "    def forward(self, waveform):\n",
    "        assert waveform.ndim == 2, waveform.shape\n",
    "        spec = self.stft(waveform)  # complex\n",
    "        spec_ri = torch.view_as_real(spec)  # (b, f, t, 2)\n",
    "        model_input = torch.permute(spec_ri, [0, 3, 2, 1])  # (b, 2, t, f)\n",
    "        model_output = self.engine(model_input)\n",
    "        # (b, 2, t, f) -> (b, f, t, 2)\n",
    "        spec_enhanced = torch.permute(model_output, [0, 3, 2, 1]).contiguous()\n",
    "        spec_enhanced = torch.view_as_complex(spec_enhanced)\n",
    "        wave_enhanced = self.istft(spec_enhanced, length=waveform.shape[-1])\n",
    "        return wave_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ce525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    engine = UNetEngine()\n",
    "    model = ComplexSpectumMappingModel(engine)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "x = torch.rand(3, 48_000)\n",
    "out = model(x)\n",
    "\n",
    "assert x.shape == out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c2eee",
   "metadata": {},
   "source": [
    "# Loss function [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c1dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Operates on waveforms.\n",
    "    \n",
    "    Computes spectrograms from them and evaluates complex spectral and magnitude losses.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fft=1024,\n",
    "        win_size: int | None = None,\n",
    "        hop_size: int | None = None,\n",
    "        mult_complex: float = 0.3,\n",
    "        criterion: (\n",
    "            tp.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None\n",
    "        ) = None,\n",
    "        window_fn=torch.hann_window,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stft = tha.transforms.Spectrogram(\n",
    "            n_fft=n_fft,\n",
    "            win_length=win_size,\n",
    "            hop_length=hop_size,\n",
    "            window_fn=window_fn,\n",
    "            power=None,\n",
    "        )\n",
    "        if criterion is None:\n",
    "            criterion = nn.MSELoss()\n",
    "        self.criterion = criterion\n",
    "        assert 0 <= mult_complex <= 1, mult_complex\n",
    "        self.mult_complex = mult_complex\n",
    "\n",
    "    def forward(self, waveform_est: torch.Tensor, waveform_target: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Apply self.stft to est and target to get complex spectrograms.\n",
    "        Compute self.criterion on the complex spectrograms.\n",
    "        \n",
    "        Compute magnitude spectograms and compute self.criterion on them.\n",
    "        \n",
    "        The final loss should be the sum of magnitude and complex parts.\n",
    "        \"\"\"\n",
    "        assert est.ndim == 2, (est.shape, \"batched\")\n",
    "        assert target.ndim == 2, (target.shape, \"batched\")\n",
    "        # your code\n",
    "        return loss_final, loss_complex, loss_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SpectralLoss()\n",
    "\n",
    "est = torch.rand(3, 48_000)\n",
    "target = torch.rand(3, 48_000)\n",
    "\n",
    "criterion(est, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69585dbd",
   "metadata": {},
   "source": [
    "# 6. Train loop [3 points]\n",
    "\n",
    "Now everything is ready. Let's check if out model can learn something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16347c6",
   "metadata": {},
   "source": [
    "General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d19b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 2  # parallel data generation\n",
    "\n",
    "LR = 2e-4\n",
    "BATCH_SIZE = 10  # the bigger the better\n",
    "MAX_GRAD_NORM = 4  # for clipping\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    PIN_MEMORY = True\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    PIN_MEMORY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b37d9",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab22419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.audio import SignalNoiseRatio, ScaleInvariantSignalNoiseRatio\n",
    "\n",
    "\n",
    "metrics = torch.nn.ModuleDict(\n",
    "    {\n",
    "        \"SNR\": SignalNoiseRatio(),\n",
    "        \"SI-SNR\": ScaleInvariantSignalNoiseRatio(),\n",
    "    }\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f74ee3",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0675a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SNR_RANGE = (-5, 5)  # range for SNR in data generation\n",
    "CROP_SIZE_SEC = 5\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "split_speech = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"speech\"]),\n",
    "    random_state=2967,\n",
    "    test_size=test_size,\n",
    ")\n",
    "split_noise = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"noise\"]),\n",
    "    random_state=8701,\n",
    "    test_size=test_size,\n",
    ")\n",
    "split_rir = train_test_split(\n",
    "    list_wavs_in_folder_recursively(DATA_PATHS[\"rir\"]),\n",
    "    random_state=9807,\n",
    "    test_size=test_size,\n",
    ")\n",
    "\n",
    "\n",
    "loaders = {}\n",
    "for split_idx, mode in enumerate([\"train\", \"val\"]):\n",
    "    sig_sampler = SignalSampler(split_speech[split_idx], crop_size_sec=crop_size_sec)\n",
    "    noise_sampler = SignalSampler(split_noise[split_idx], crop_size_sec=crop_size_sec)\n",
    "    rir_sampler = RirSampler(split_rir[split_idx])\n",
    "    mixture_sampler = RandomMixtureSampler(\n",
    "        sig_sampler=sig_sampler,\n",
    "        noise_sampler=noise_sampler,\n",
    "        rir_sampler=rir_sampler,\n",
    "        prob_rir_noise=0.5,\n",
    "        prob_rir_sig=0.5,\n",
    "        normalization_rms_db=-20,\n",
    "        noise_gain_range_db=(-SNR_RANGE[1], -SNR_RANGE[0]),\n",
    "        mixture_gain_range_db=(-25, 5),\n",
    "        sr=SR,\n",
    "        partial_dereverb=True,\n",
    "    )\n",
    "    batches_per_epoch = 1024 if mode == \"train\" else 256\n",
    "    dataset = Dataset(mixture_sampler, dummy_duration=BATCH_SIZE * batches_per_epoch)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    loaders[mode] = loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe7640",
   "metadata": {},
   "source": [
    "model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = SpectralLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d9173",
   "metadata": {},
   "source": [
    "Tensorboard logger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfcb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "SAVE_SOUND_FREQ = 512\n",
    "LOG_FREQ = 4\n",
    "SAVE_SNAPSHOT_FREQ = 512\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, training: bool, global_step_idx: int = 0):\n",
    "    mode_name = \"train\" if training else \"val\"\n",
    "    \n",
    "    for evaluator in metrics.values():\n",
    "        evaluator.reset()\n",
    "    loss_storage = [[] for _ in range(3)]\n",
    "    for step_idx, (mixture, target) in enumerate(tqdm(loader)):\n",
    "        mixture = mixture.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        est = # your code: run the model\n",
    "\n",
    "        loss_components = criterion(est, target)\n",
    "        loss = loss_components[0]\n",
    "\n",
    "        for storage, component in zip(loss_storage, loss_components):\n",
    "            storage.append(component.item())\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for name, evaluator in metrics.items():\n",
    "                value_out = evaluator(est, target).mean().item()\n",
    "            \n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), MAX_GRAD_NORM\n",
    "            )\n",
    "            optimizer.step()\n",
    "\n",
    "        if (step_idx % LOG_FREQ == 0) and training:\n",
    "            loss_logs = [sum(x) / len(x) for x in loss_storage]\n",
    "            writer.add_scalar(f\"{mode_name}/loss/final\", loss_logs[0], global_step_idx)\n",
    "            writer.add_scalar(f\"{mode_name}/loss/complex\", loss_logs[1], global_step_idx)\n",
    "            writer.add_scalar(f\"{mode_name}/loss/magnitude\", loss_logs[2], global_step_idx)\n",
    "            loss_storage = [[] for _ in range(3)]\n",
    "\n",
    "            writer.add_scalar(f\"{mode_name}/grad_norm\", grad_norm.item(), global_step_idx)\n",
    "\n",
    "            for name, evaluator in metrics.items():\n",
    "                value_out = evaluator.compute().item()\n",
    "                writer.add_scalar(f\"{mode_name}/metrics/{name}\", value_out, global_step_idx,)\n",
    "                evaluator.reset()\n",
    "\n",
    "        if step_idx % SAVE_SOUND_FREQ == 0:\n",
    "            with torch.no_grad():\n",
    "                path_samples = f\"samples/{mode_name}\"\n",
    "                os.makedirs(path_samples, exist_ok=True)\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_mixture.wav\",\n",
    "                    mixture[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_clean.wav\",\n",
    "                    target[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_noise.wav\",\n",
    "                    mixture[0].cpu().numpy() - target[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "                sf.write(\n",
    "                    f\"{path_samples}/{global_step_idx:05d}_est.wav\",\n",
    "                    est[0].cpu().numpy(),\n",
    "                    SR,\n",
    "                )\n",
    "        if step_idx % SAVE_SNAPSHOT_FREQ == 0 and training:\n",
    "            torch.save(model.state_dict(), \"state_dict_latest.pt\")\n",
    "        if training:\n",
    "            global_step_idx += 1\n",
    "            \n",
    "    if not training:\n",
    "        for name, evaluator in metrics.items():\n",
    "            value_out = evaluator.compute().item()\n",
    "            print(f\"{mode_name}/metrics/{name}\", value_out)\n",
    "            writer.add_scalar(f\"{mode_name}/metrics/{name}\", value_out, global_step_idx,)\n",
    "        \n",
    "    return global_step_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c9943",
   "metadata": {},
   "source": [
    "Train loop itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ec7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step_idx = 0\n",
    "while True:\n",
    "    global_step_idx = run_epoch(loaders[\"train\"], True, global_step_idx)\n",
    "    with torch.no_grad():\n",
    "        run_epoch(loaders[\"val\"], False, global_step_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bda01",
   "metadata": {},
   "source": [
    "### What to expect?\n",
    "\n",
    "SNR and SI-SNR should go up on both train and validation. They should reach 1-2 dB in a couple dozens minutes. In 8 hours on a 1080ti GPU they should reach around 10 dB. The model starts with random predictions, so the initial growth is noticeable since the first minutes, however it does not mean that the model was implemented correctly.\n",
    "\n",
    "### Scoring\n",
    "5dB SI-SNR on train test should be beaten to complete the training loop part. Points for main loop will be given based on the final SI-SDR. 10 dB on validation set will definitely result in full points.\n",
    "\n",
    "### What if the hardware is not powerful enough?\n",
    "Colab instances should be power enough. However, if it doesn't work for you, please, try to tune the parameters and make the network smaller. If the models are still too big, try [grouped RNN](https://arxiv.org/pdf/2101.09249.pdf). [This model](https://arxiv.org/pdf/2008.06412.pdf) should be cheaper in computation (with a worse quality). Finally there is a bonus task for streaming inference ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683d93f",
   "metadata": {},
   "source": [
    "# Bonus: streaming inference\n",
    "\n",
    "The model you implemented above should be streaming-friendly.\n",
    "\n",
    "During classwork we implemented streaming STFT and ISTFT with a toy VQE. In this assigment you are required to implement streaming end-to-end inference for your network, i.e. take 10-ms chunks of audio as input and output 10-ms chunks of enhanced audio.\n",
    "\n",
    "Probably you will re-implement your model in PyTorch and load an adapted version of state_dict from the offline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d905b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
